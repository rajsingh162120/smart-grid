import pandas as pd
df= pd.read_csv("wind_power_combined.csv")
df



from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Features (X) are 'Air temperature | (°C)', 'Pressure | (atm)', and 'Wind speed | (m/s)'
# Target (y) is 'Power generated by system | (MW)'

# Extracting features (X) and target (y)
X = df[['Air temperature | (°C)', 'Pressure | (atm)', 'Wind speed | (m/s)']]
y = df['Power generated by system | (MW)']

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Splitting the scaled data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize the Gradient Boosting regressor
gb_regressor = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Training the Gradient Boosting regressor
gb_regressor.fit(X_train, y_train)

# Predicting on the test set
y_pred = gb_regressor.predict(X_test)

# Evaluating the model using metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:", mse)
print("R-squared:", r2)



model=gb_regressor


import joblib


model_file_path = "gradient_boosting_model.pkl"

joblib.dump(model, model_file_path)



import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

def plot_learning_curve(estimator, X, y, train_sizes=np.linspace(0.1, 1.0, 5), cv=5):
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, train_sizes=train_sizes, cv=cv, scoring='neg_mean_squared_error')
    
    train_scores_mean = -np.mean(train_scores, axis=1)
    test_scores_mean = -np.mean(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_scores_mean, label='Training error')
    plt.plot(train_sizes, test_scores_mean, label='Validation error')
    plt.xlabel('Training examples')
    plt.ylabel('Mean Squared Error')
    plt.title('Learning Curve')
    plt.legend()
    plt.grid(True)
    plt.show()

# Plot learning curve
plot_learning_curve(gb_regressor, X_scaled, y)



# Load the forecasted data for 2024
forecasted_data_2024 = pd.read_excel("wind_test_data.xlsx")

# Extracting features for prediction
X_forecasted = forecasted_data_2024[['Air temperature | (°C)', 'Pressure | (atm)', 'Wind speed | (m/s)']]

# Scale the features if necessary
X_forecasted_scaled = scaler.transform(X_forecasted)  # Assuming 'scaler' is the StandardScaler object used for scaling

# Predict power generation using the trained Gradient Boosting regressor
predictions = gb_regressor.predict(X_forecasted_scaled)

# Print or save the predictions
print("Predicted power generation for 2024:")
print(predictions)



predictions.sum()


# Add the predictions to the forecasted data DataFrame
forecasted_data_2024['Predicted Power Generation | (MW)'] = predictions

# Save the DataFrame with predictions to a new Excel file
forecasted_data_2024.to_csv("gbr_forecasted_power_generation_2024.csv", index=False)

df_csv = pd.read_csv("gbr_forecasted_power_generation_2024.csv")
df_csv.to_excel("gbr_forecasted_power_generation_2024.xlsx")

print("Forecasted data with predictions for 2024:")
print(forecasted_data_2024)




# Step 1: Load the data
data = pd.read_csv("final_data.csv")

# Step 2: Calculate the total power generated
total_power_generated = data['p1'].sum() + data['p2'].sum() + data['p3'].sum()

# Step 3: Distribute the power to each node based on the specified percentages
power_node1 = total_power_generated * 0.20  # 20% of total power
power_node2 = total_power_generated * 0.45  # 45% of total power
power_node3 = total_power_generated * 0.35  # 35% of total power

# Step 4: Prepare a DataFrame to store the distributed power for each node
node_data = {
    'Node': ['Node 1', 'Node 2', 'Node 3'],
    'Power Generated (MW)': [power_node1, power_node2, power_node3]
}

# Create a DataFrame
power_distribution_df = pd.DataFrame(node_data)

# Calculate the total power generated including P1, P2, and P3
total_power_generated_all = total_power_generated + data['p1'].sum() + data['p2'].sum() + data['p3'].sum()

# Check if generated and distributed powers are equal
equal_powers = total_power_generated == (power_node1 + power_node2 + power_node3)

# Display the DataFrame
print(power_distribution_df)

# Display the total power generated
print("Total Power Generated (including P1, P2, P3):", total_power_generated_all, "MW")

# Display if generated and distributed powers are equal
print("Generated and Distributed Powers are Equal:", equal_powers)



total_power_generated_all


# Step 1: Create new columns for each node
data['Power_Node_1'] = data['Power generated by system | (MW)'] * 0.20  # 20% of total power
data['Power_Node_2'] = data['Power generated by system | (MW)'] * 0.45  # 45% of total power
data['Power_Node_3'] = data['Power generated by system | (MW)'] * 0.35  # 35% of total power

# Step 2: Optionally, drop the original columns
data.drop(columns=['Power generated by system | (MW)'], inplace=True)

# Step 3: Display the updated DataFrame
print(data)



data.head()


data.to_csv("gbr_updated_dataset.csv")

df_csv = pd.read_csv("gbr_updated_dataset.csv")
df_csv.to_excel("gbr_updated_dataset.xlsx")



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report

# Split the dataset into features and target
X = data.drop(columns=['stability', 'date'])  
y = data['stability']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the Gradient Boosting classifier
gradient_boosting_model = GradientBoostingClassifier(n_estimators=100, random_state=42)

# Train the model
gradient_boosting_model.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred_gb = gradient_boosting_model.predict(X_test_scaled)

# Calculate accuracy
accuracy_gb = accuracy_score(y_test, y_pred_gb)
print("Test Accuracy (Gradient Boosting):", accuracy_gb)

# Print classification report
print("Classification Report (Gradient Boosting):")
print(classification_report(y_test, y_pred_gb))



import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv("wind_power_combined.csv")

# Features (X) are 'Air temperature | (°C)', 'Pressure | (atm)', and 'Wind speed | (m/s)'
# Target (y) is 'Power generated by system | (MW)'
X = df[['Air temperature | (°C)', 'Pressure | (atm)', 'Wind speed | (m/s)']]
y = df['Power generated by system | (MW)']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Scale the features
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the Gradient Boosting regressor
gb_regressor = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Training the Gradient Boosting regressor
gb_regressor.fit(X_train_scaled, y_train)

# Predicting on the test set
y_pred = gb_regressor.predict(X_test_scaled)



# Define custom input data
custom_input = {
    'Air temperature | (°C)': [2.6, 6.8, 5.9],
    'Pressure | (atm)': [0.9932, 0.0981, 0.9765],
    'Wind speed | (m/s)': [11.5, 11.02, 10.5]          
}

# Create a DataFrame with custom input data
custom_input_df = pd.DataFrame(custom_input)

# Scale the custom input data using the same scaler
custom_input_scaled = scaler.transform(custom_input_df)

# Get the predictions using custom input data
custom_predictions = gb_regressor.predict(custom_input_scaled)

# Print the predictions
print("Predicted power generation for custom inputs:")
print(custom_predictions)



import pandas as pd


# Step 1: Convert the 'date' column to datetime format
data['date'] = pd.to_datetime(data['date'], format="%d-%m-%Y %H:%M")

# Step 2: Calculate the percentage of 'Stable' and 'Unstable' grid conditions
stability_percentage = data['stability'].value_counts(normalize=True) * 100
print("Percentage of Stable Grid Conditions:", stability_percentage['stable'], "%")
print("Percentage of Unstable Grid Conditions:", stability_percentage['unstable'], "%")

# Step 3: Identify patterns in the hours when the grid is most prone to instability
# Extract hour from the date column
data['Hour'] = data['date'].dt.hour
unstable_hours = data[data['stability'] == 'unstable']['Hour'].value_counts()
most_unstable_hour = unstable_hours.idxmax()
print("Hour when the grid is most prone to instability:", most_unstable_hour)

# Step 4: Analyze the frequency and duration of 'Unstable' conditions throughout the day
unstable_hours_distribution = data[data['stability'] == 'unstable']['Hour'].value_counts(normalize=True) * 100
print("Distribution of Unstable Conditions throughout the day:")
print(unstable_hours_distribution)






unstable_hour_distribution = unstable_hours.sort_index()
plt.figure(figsize=(10, 6))
plt.bar(unstable_hour_distribution.index, unstable_hour_distribution.values)
plt.xlabel('Hour of Day')
plt.ylabel('Proportion of Unstable Conditions')
plt.title('Distribution of Unstable Conditions Throughout the Day')
plt.xticks(range(24))
plt.grid(True)
plt.show()


# Step 5: Visualize the trend of stable and unstable conditions over time
data['Date'] = data['date'].dt.date
stability_trend = data.groupby(['Date', 'stability']).size().unstack(fill_value=0)
stability_trend.plot(kind='line', figsize=(12, 6))
plt.title('Stability Trend Over Time')
plt.xlabel('Date')
plt.ylabel('Frequency')
plt.legend(title='Stability', loc='upper left')
plt.grid(True)
plt.show()



import pandas as pd
import matplotlib.pyplot as plt

# Load the sample data
data = pd.read_csv("final_data.csv")  
# Convert 'date' column to datetime format
data['date'] = pd.to_datetime(data['date'], format='%d-%m-%Y %H:%M')

# Extract year and quarter from the 'date' column
data['Year'] = data['date'].dt.year
data['Quarter'] = data['date'].dt.to_period('Q')

# Group by year and quarter and calculate total power generated for each quarter
power_by_year_quarter = data.groupby(['Year', 'Quarter'])['Power generated by system | (MW)'].sum().reset_index()

# Plotting the total power generated for each quarter over the years
plt.figure(figsize=(12, 8))
for year in range(2019, 2024):
    year_data = power_by_year_quarter[power_by_year_quarter['Year'] == year]
    plt.plot(year_data['Quarter'].astype(str), year_data['Power generated by system | (MW)'],
             label=str(year))

plt.title('Total Power Generated for Each Quarter (2019-2023)')
plt.xlabel('Quarter')
plt.ylabel('Total Power Generated (MW)')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.grid(True)
plt.legend(title='Year')
plt.tight_layout()
plt.show()



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
data = pd.read_csv("final_data.csv")

# Select variables for correlation analysis
variables = ['Air temperature | (°C)', 'Pressure | (atm)', 'Wind speed | (m/s)', 'Power generated by system | (MW)']

# Create a pairplot to visualize relationships and distributions
sns.pairplot(data[variables])
plt.show()

# Calculate correlation matrix
correlation_matrix = data[variables].corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 10})
plt.title('Correlation Matrix')
plt.show()

# Print correlation coefficients
print("Correlation coefficients:")
print(correlation_matrix)



from joblib import dump, load

# Save the trained model
dump(model, 'gradient_boosting_model.joblib')

# Load the trained model
model = load('gradient_boosting_model.joblib')



# Prepare the input data for prediction
new_data = [[6.6, 0.98, 10.8]]  # Replace feature1, feature2, ... with actual values

# Make predictions
predictions = model.predict(new_data)



# Make predictions
predictions = model.predict(new_data)

# Display predictions
print("Predicted power generation:", predictions)



import streamlit as st
import joblib
import numpy as np

# Load the trained model
model = joblib.load('gradient_boosting_model.joblib')

# Define the UI elements
st.title('Power Generation Prediction')
st.sidebar.header('Input Parameters')

# Define input fields for user input
feature1 = st.sidebar.number_input('Feature 1')
feature2 = st.sidebar.number_input('Feature 2')
feature3 = st.sidebar.number_input('Feature 3')

# Prepare the input data for prediction
input_data = np.array([[feature1, feature2, feature3]])

# Make prediction
prediction = model.predict(input_data)

# Display the prediction
st.write('Predicted Power Generation:', prediction)



streamlit run gbr_model.ipynb




